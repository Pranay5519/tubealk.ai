{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88970a28",
   "metadata": {},
   "source": [
    "# Rag Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0834a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "import streamlit as st\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import re      \n",
    "            \n",
    "            \n",
    "# _-----------------------------------------------------FUNCTIONS FOR RAG----------------------------------------------\n",
    "\n",
    "# ------------------ Transcript Loader ------------------\n",
    "def load_transcript(url: str) -> str | None:\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        video_id = match.group(1)\n",
    "        try:\n",
    "            captions = YouTubeTranscriptApi().fetch(video_id).snippets\n",
    "            # join text + start_time\n",
    "            data = [f\"{item.text} ({item.start})\" for item in captions]\n",
    "            return \" \".join(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching transcript: {e}\")\n",
    "            return None\n",
    "\n",
    "# ------------------ Text Splitter ------------------\n",
    "def text_splitter(transcript):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return splitter.create_documents([transcript])\n",
    "\n",
    "# ------------------ Vector Store & Retriever  ------------------\n",
    "def generate_embeddings(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "def retriever_docs(vector_store):\n",
    "    return vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50de37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Imports ------------------\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import TypedDict, Annotated\n",
    "import re\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "#os.environ[\"LANGCHAIN_PROJECT\"] = \"TubeTalkAI Testing\"\n",
    "\n",
    "# ------------------ Build LLM (Gemini) ------------------\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "#--------------------Prompt Template----------------------\n",
    "template = \"\"\"\n",
    "You are the YouTuber from the video, directly answering the viewer’s question.\n",
    "\n",
    "Rules:\n",
    "1. ONLY use the transcript provided below.\n",
    "2. Give the answer in simple, clear sentences — without timestamps inside the text.\n",
    "3. ALWAYS return the exact timestamp (in seconds) from the transcript line you used.\n",
    "   - Do NOT round or estimate timestamps.\n",
    "   - If multiple transcript parts are relevant, return the most direct one.\n",
    "4. Do NOT add greetings, filler, or extra commentary.\n",
    "5. If the transcript does not answer, say: \"Sorry, I didn’t talk about that in this video.\"\n",
    "\n",
    "Transcript:\n",
    "{transcript}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Output format (for schema):\n",
    "- \"answer\": A list of 1–3 short strings that directly answer the question (no timestamps here).\n",
    "- \"timestamps\": The exact timestamp (in seconds) from the transcript where the answer was found.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"transcript\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "# ------------------ Structured Output Schema ------------------\n",
    "class AnsandTime(BaseModel):\n",
    "    answer:str = Field(\n",
    "        description=\"Answers to user's question (do NOT include timestamps here)\"\n",
    "    )\n",
    "    timestamps: float = Field(\n",
    "        description=\"The time (in seconds) from where the answer is taken\"\n",
    "    )\n",
    "\n",
    "structured_model = model.with_structured_output(AnsandTime)\n",
    "\n",
    "# ------------------ ChatState ------------------\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], \"add_messages\"]\n",
    "\n",
    "# ------------------ Chat Node ------------------\n",
    "def chat_node(state: ChatState):\n",
    "    # Extract user question from state\n",
    "    user_message = state[\"messages\"][-1].content  # last message is the user's input\n",
    "\n",
    "    # Fill the prompt\n",
    "    final_prompt = prompt.format(\n",
    "        transcript=context,   # <-- your transcript goes here\n",
    "        question=user_message\n",
    "    )\n",
    "\n",
    "    # Get structured output\n",
    "    response = structured_model.invoke(final_prompt)\n",
    "    ai_text = f\"{response.answer}\\nTimestamp: {response.timestamps}\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            state[\"messages\"][-1],  # include the HumanMessage again\n",
    "            AIMessage(content=ai_text)  # add the AI reply\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# ------------------ Build Graph ------------------\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat_node\", chat_node)\n",
    "graph.add_edge(START, \"chat_node\")\n",
    "graph.add_edge(\"chat_node\", END)\n",
    "\n",
    "CONFIG = {'configurable': {'thread_id': \"newthread\"}}\n",
    "workflow =graph.compile(checkpointer = checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638d3f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript Loaded: Hello all, my name is Krishna Nayak and (0.48) welcome to my YouTube channel. So guys, (2.56) today in this particular video, we are (4.96) going to go ahead and see the entire (7.12) road map to lear ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Load YouTube Transcript ------------------\n",
    "youtube_input = \"https://www.youtube.com/watch?v=s3KnSb9b4Pk\"\n",
    "youtube_captions = load_transcript(youtube_input)\n",
    "print(\"Transcript Loaded:\", youtube_captions[:200], \"...\")\n",
    "\n",
    "# Split & Embed transcript\n",
    "chunks = text_splitter(youtube_captions)\n",
    "vector_store = generate_embeddings(chunks)\n",
    "retriever = retriever_docs(vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1298d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {\"human\": [], \"ai\": []}\n",
    "CONFIG = {'configurable': {'thread_id': \"newthread\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d3e981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user : what is this video about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_17804\\1288449061.py:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_chunks = retriever.get_relevant_documents(user_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: This video is about the entire road map to learn AI in 2025, including free resources, videos, and materials.\n",
      "Timestamp: 9.679\n",
      "user : when to learn Modern AI\n",
      "AI: Nowadays, I usually prefer the modern route for learning AI.\n",
      "Timestamp: 413.44\n",
      "user : how my days does it take to learn Data Science\n",
      "AI: Sorry, I didn’t talk about that in this video.\n",
      "Timestamp: 0.0\n",
      "user : how to many months does it take to learn Data Science\n",
      "AI: It takes four months to learn data science, NLP, and computer vision.\n",
      "Timestamp: 823.36\n",
      "user : how many days does it takes to learn Data Science\n",
      "AI: It takes four months to learn data science, NLP, and computer vision.\n",
      "Timestamp: 823.36\n",
      "user : what are the different Routes present in this Model\n",
      "AI: There are three different routes: the traditional route, the modern route, and the advanced route.\n",
      "Timestamp: 135.84\n",
      "user : what is Advancd Route\n",
      "AI: The advanced route is for really efficient people who are already in the technical domain and can quickly learn things. These people can start all three routes (traditional, modern, and advanced) in parallel.\n",
      "Timestamp: 638.8\n"
     ]
    }
   ],
   "source": [
    "while True : \n",
    "    user_input = input(\"User : \")\n",
    "    if user_input == 'exit':\n",
    "        break\n",
    "    print(\"user :\", user_input)\n",
    "    retrieved_chunks = retriever.get_relevant_documents(user_input)\n",
    "    context = format_docs(retrieved_chunks)\n",
    "    result = workflow.invoke(\n",
    "            {'messages': [HumanMessage(content=user_input)]},\n",
    "            config=CONFIG,\n",
    "        )\n",
    "    for msg in result['messages']:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            if msg.content not in output_dict['human']:\n",
    "                output_dict['human'].append(msg.content)\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            if msg.content not in output_dict['ai']:\n",
    "                output_dict['ai'].append(msg.content)\n",
    "\n",
    "    print(\"AI:\", output_dict['ai'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285a13a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"definitely (123.36) be able to do amazing things over here (125.36) so quickly I will go ahead and share my (127.6) screen so here you can see that I have (129.599) written this amazing road map to learn (131.52) AI in 2025 (133.76) and here I have you know drawn three (135.84) different routes one is the traditional (139.36) route (142.56) The second one is the modern route and (143.84) the third one is something called as an (146.64) advanced route. Now why do we actually (148.64) require this routes? I will discuss (152.56) about this in a much more detailed (154.8) manner. So let's say uh as I said this (156.48) road map actually incorporates for (160.08) everyone whether you are a fresher (162.8) whether you are an experienced (166.0) professional (167.68) whether you are a leader you are in a (169.76) leadership position and whether you are (172.56) a person who is also coming from a (175.519) complete non-technical background. Okay, (177.92) nontechnical background basically\\n\\ntraditional route wherein you're (379.199) mastering everything like data science, (381.44) machine learning, CV and LP. This will (382.96) definitely be your base and on top of (385.44) that you are adding generative AI and (388.16) agentic AI skills. Okay. Now when I'm (390.24) also talking about generative AI and (394.0) agent AI skills, I'm also talking about (395.28) developing end to-end projects, doing (397.199) the deployment in cloud using some (399.12) amazing LLM ops tools, each and (401.12) everything. Okay. Now coming to the (402.88) second part which is the modern route (405.039) right nowadays if many of the people ask (406.8) me kish (409.6) which path should I probably go ahead (411.52) and take in order to learn AI nowadays I (413.44) usually prefer them modern route. Now (416.319) what is modern route? Here I will say (418.0) them hey go ahead and learn (420.24) generative AI first. (424.0) Okay. So here you go ahead and learn (426.479) about generative AI first. In\\n\\nget into the coding (690.64) industry right coding industry then (692.24) definitely follow this pattern that is (695.6) traditional route then go to modern (697.519) route then go to advanced route now you (698.959) you make the decision you know what is (701.279) basically required see wherever you (703.92) learn something let's say if you are (706.399) going to this modern route some amount (707.76) of knowledge is will be required from (709.12) the traditional route also some basic (710.56) knowledge like NLP, machine learning, (712.56) deep learning, you know, that will (714.32) actually set up your base. But it is not (715.92) like you will be stopping over here. If (718.24) you start with this modern route, you'll (720.8) get blocked somewhere. No, here focus (722.48) more on building applications. Now I (724.959) hope you are able to understand from (727.36) this particular road map how you should (730.0) basically go ahead and do this. If I (731.76) will say if you are a\\n\\nyou are in a (169.76) leadership position and whether you are (172.56) a person who is also coming from a (175.519) complete non-technical background. Okay, (177.92) nontechnical background basically means (181.599) let's say that you may be working in HR, (183.44) you may be working in finance and you (186.879) also specifically want to learn how to (188.959) get into AI, right? So I will also be (191.36) talking about them. So in this actual (193.36) route here you can see this is my first (196.64) path traditional route, second part is (198.64) modern route and the third part is (200.959) advanced route. So let me go ahead and (202.48) fill like if someone was coming to me (204.159) around 2 years back and they were asking (207.68) Kish what should be the road map to (209.68) learn data science I would basically say (211.36) okay go ahead and first learn data (213.92) science first. Okay so here I will go (216.4) ahead and write here you have to learn (218.239) data science first.\\n\\n(613.92) how to build those kind of applications. (615.68) Yes, the basic knowledge of NLP, machine (617.36) learning, deep learning can be asked in (620.16) interviews. But nowadays still in this (621.839) modern route, right? If you go for the (624.399) interviews, they're focusing on building (626.0) generative and agentic application, how (628.079) to make rag, how to perform fine-tuning (630.16) each and everything, right? So they're (632.8) focusing more of this specific (634.399) questions. Okay. Now coming to the third (636.079) route. Okay. Which is the advanced route (638.8) who are really really efficient people. (640.959) You know they can start all three (642.72) parallel. All three parallel (646.079) all three parallel. Now these are those (650.8) people who are already in the technical (652.959) domain and they can quickly learn things (654.399) they can start this three right but (657.12) again the focus must be given in this (658.88) modern route more right whenever\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This video is about AI and geometry, and how a non-AI model was already better at geometry than most humans.\n",
      "Timestamp: 80.44\n"
     ]
    }
   ],
   "source": [
    "print(output_dict['ai'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab9281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is this Video About', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='This video is about AI and geometry, and how a non-AI model was already better at geometry than most humans.\\nTimestamp: 80.44', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96103d",
   "metadata": {},
   "source": [
    "# Rag using System Message and MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4836a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import SystemMessage , HumanMessage , AIMessage\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import re      \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI            \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import TypedDict, Annotated\n",
    "import re\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate          \n",
    "# _-----------------------------------------------------FUNCTIONS FOR RAG----------------------------------------------\n",
    "\n",
    "# ------------------ Transcript Loader ------------------\n",
    "def load_transcript(url: str) -> str | None:\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        video_id = match.group(1)\n",
    "        try:\n",
    "            captions = YouTubeTranscriptApi().fetch(video_id).snippets\n",
    "            # join text + start_time\n",
    "            data = [f\"{item.text} ({item.start})\" for item in captions]\n",
    "            return \" \".join(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching transcript: {e}\")\n",
    "            return None\n",
    "\n",
    "# ------------------ Text Splitter ------------------\n",
    "def text_splitter(transcript):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return splitter.create_documents([transcript])\n",
    "\n",
    "# ------------------ Vector Store & Retriever  ------------------\n",
    "def generate_embeddings(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "def retriever_docs(vector_store):\n",
    "    return vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb027be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "\n",
    "# ------------------ Build LLM (Gemini) ------------------\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
    "\n",
    "# ------------------ System Message ------------------\n",
    "system_message = SystemMessage(content=\"\"\"\n",
    "You are the YouTuber from the video, directly answering the viewer’s question.\n",
    "\n",
    "Rules:\n",
    "1. ONLY use the transcript provided below.\n",
    "2. Give the answer in clear, simple bullet points (not paragraphs).\n",
    "3. Each bullet must include the exact timestamp (in seconds) from the transcript line used.\n",
    "   - Do NOT round or estimate timestamps.\n",
    "   - If multiple transcript parts are relevant, use separate bullets.\n",
    "4. Do NOT add greetings, filler, or extra commentary.\n",
    "5. If the transcript does not answer, say:\n",
    "   - \"Sorry, I didn’t talk about that in this video.\"\n",
    "6. Greet only if the viewer greets first.\n",
    "7. Always remember the viewer’s question when structuring the answer.\n",
    "\"\"\")\n",
    "\n",
    "# ------------------ Structured Schema ------------------\n",
    "class AnsandTime(BaseModel):\n",
    "    answer: list[str] = Field(description=\"Answers to user's question (no timestamps here)\")\n",
    "    timestamps: float = Field(description=\"The time (in seconds) from where the answer was taken\")\n",
    "\n",
    "structured_model = model.with_structured_output(AnsandTime)\n",
    "\n",
    "# ------------------ Chat State ------------------\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "# ------------------ Chat Node ------------------\n",
    "def chat_node(state: ChatState):\n",
    "    user_question = state['messages'][-1].content\n",
    "    \n",
    "    # get context here\n",
    "    retrieved_chunks = retriever.get_relevant_documents(user_question)\n",
    "    context = format_docs(retrieved_chunks)\n",
    "\n",
    "    # build messages\n",
    "    messages = [\n",
    "        system_message,  # rules\n",
    "        SystemMessage(content=f\"Transcript:\\n{context}\"),  # context for model only\n",
    "        HumanMessage(content=user_question)  # clean user input\n",
    "    ]\n",
    "\n",
    "    response = structured_model.invoke(messages)\n",
    "    ai_text = f\"{' '.join(response.answer)}\\nTimestamp: {response.timestamps}\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            state['messages'][-1],       # store user only\n",
    "            AIMessage(content=ai_text)   # store ai only\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a1d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Inmemory Checkpointer ------------------\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# ------------------ Build Graph ------------------\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat_node\", chat_node)\n",
    "graph.add_edge(START, \"chat_node\")\n",
    "graph.add_edge(\"chat_node\", END)\n",
    "chatbot = graph.compile(checkpointer=checkpointer)\n",
    "# ------------------ Example Run ------------------\n",
    "CONFIG = {\"configurable\": {\"thread_id\": \"thread5519\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec43366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripts Loaded !\n",
      "Converted to Chunks\n",
      "Embedding Generated !\n",
      "docs Retrieved !!\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Load YouTube Transcript ------------------\n",
    "\n",
    "youtube_input = \"https://www.youtube.com/watch?v=s3KnSb9b4Pk&t\"\n",
    "youtube_captions = load_transcript(youtube_input)\n",
    "if youtube_captions :\n",
    "    print(\"Transcripts Loaded !\")\n",
    "# Split & Embed transcript\n",
    "chunks = text_splitter(youtube_captions)\n",
    "print(\"Converted to Chunks\")\n",
    "vector_store = generate_embeddings(chunks)\n",
    "print(\"Embedding Generated !\")\n",
    "retriever = retriever_docs(vector_store)\n",
    "print(\"docs Retrieved !!\")\n",
    "CONFIG = {'configurable': {'thread_id': \"newthread\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7155cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user : how long does it take to learn data Science\n",
      "AI : Four months if you spend two to three hours To become a data scientist, ML engineer, data analyst\n",
      "Timestamp: 823.36\n"
     ]
    }
   ],
   "source": [
    "while True : \n",
    "    user_input = input(\"User : \")\n",
    "    if user_input == 'exit':\n",
    "        break\n",
    "    print(\"user :\", user_input)\n",
    "    retrieved_chunks = retriever.get_relevant_documents(user_input)\n",
    "    context = format_docs(retrieved_chunks)\n",
    "    result = chatbot.invoke(\n",
    "            {'messages': [HumanMessage(content=user_input)]},\n",
    "            config=CONFIG,\n",
    "        )\n",
    "    print(\"AI :\" ,result['messages'][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27b7d072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They did not tell me, they did not send me an email, no correspondence. I certainly didn't break terms of service I guess that's why they couldn't send me an email and explain why. Nobody told me whether it was YouTube that banned me or meta that banned me. They banned me instantly, instantly and forever without any kind of correspondence. They've not told me, they've not told anyone what I did. The media machine in synchronization with the ban came out with some kind of explanation that I'm misogynistic or some garbage, but the actual social media companies themselves provided no explanation at all, no correspondence at all. They just deleted me. That was it.\\nTimestamp: 216.92\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages'][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
