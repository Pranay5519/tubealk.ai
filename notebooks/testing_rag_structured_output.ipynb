{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57002328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "import streamlit as st\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import re      \n",
    "            \n",
    "            \n",
    "# _-----------------------------------------------------FUNCTIONS FOR RAG----------------------------------------------\n",
    "\n",
    "# ------------------ Transcript Loader ------------------\n",
    "def load_transcript(url: str) -> str | None:\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        video_id = match.group(1)\n",
    "        try:\n",
    "            captions = YouTubeTranscriptApi().fetch(video_id).snippets\n",
    "            # join text + start_time\n",
    "            data = [f\"{item.text} ({item.start})\" for item in captions]\n",
    "            return \" \".join(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching transcript: {e}\")\n",
    "            return None\n",
    "\n",
    "# ------------------ Text Splitter ------------------\n",
    "def text_splitter(transcript):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return splitter.create_documents([transcript])\n",
    "\n",
    "# ------------------ Vector Store & Retriever  ------------------\n",
    "def generate_embeddings(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "def retriever_docs(vector_store):\n",
    "    return vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a097959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Imports ------------------\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import TypedDict, Annotated\n",
    "import re\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "#os.environ[\"LANGCHAIN_PROJECT\"] = \"TubeTalkAI Testing\"\n",
    "\n",
    "# ------------------ Build LLM (Gemini) ------------------\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
    "\n",
    "#--------------------Prompt Template----------------------\n",
    "template = \"\"\"\n",
    "You are a helpful assistant.\n",
    "Answer ONLY from the provided transcript context and also mention the time given in brackets.\n",
    "If the context is insufficient, just say you don't know.\n",
    "\n",
    "Context:\n",
    "{transcript}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"transcript\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "# ------------------ Structured Output Schema ------------------\n",
    "class AnsandTime(BaseModel):\n",
    "    answer:str = Field(\n",
    "        description=\"Answers to user's question (do NOT include timestamps here)\"\n",
    "    )\n",
    "    timestamps: float = Field(\n",
    "        description=\"The time (in seconds) from where the answer is taken\"\n",
    "    )\n",
    "\n",
    "structured_model = model.with_structured_output(AnsandTime)\n",
    "\n",
    "# ------------------ ChatState ------------------\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], \"add_messages\"]\n",
    "\n",
    "# ------------------ Chat Node ------------------\n",
    "def chat_node(state: ChatState):\n",
    "    # Extract user question from state\n",
    "    user_message = state[\"messages\"][-1].content  # last message is the user's input\n",
    "\n",
    "    # Fill the prompt\n",
    "    final_prompt = prompt.format(\n",
    "        transcript=youtube_captions,   # <-- your transcript goes here\n",
    "        question=user_message\n",
    "    )\n",
    "\n",
    "    # Get structured output\n",
    "    response = structured_model.invoke(final_prompt)\n",
    "    ai_text = f\"Answer: {response.answer}\\nTimestamp: {response.timestamps}\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            state[\"messages\"][-1],  # include the HumanMessage again\n",
    "            AIMessage(content=ai_text)  # add the AI reply\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# ------------------ Build Graph ------------------\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat_node\", chat_node)\n",
    "graph.add_edge(START, \"chat_node\")\n",
    "graph.add_edge(\"chat_node\", END)\n",
    "\n",
    "CONFIG = {'configurable': {'thread_id': \"newthread\"}}\n",
    "workflow =graph.compile(checkpointer = checkpointer)\n",
    "output_dict = {\"human\": [], \"ai\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "638d3f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript Loaded: Hello all, my name is Krishna Nayak and (0.48) welcome to my YouTube channel. So guys, (2.56) today in this particular video, we are (4.96) going to go ahead and see the entire (7.12) road map to lear ...\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Load YouTube Transcript ------------------\n",
    "youtube_input = \"https://www.youtube.com/watch?v=s3KnSb9b4Pk\"\n",
    "youtube_captions = load_transcript(youtube_input)\n",
    "print(\"Transcript Loaded:\", youtube_captions[:200], \"...\")\n",
    "\n",
    "# Split & Embed transcript\n",
    "chunks = text_splitter(youtube_captions)\n",
    "vector_store = generate_embeddings(chunks)\n",
    "retriever = retriever_docs(vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1298d821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user : When to learn Modern AU\n",
      "AI: Answer: The modern route to learning AI involves mastering generative AI first, focusing on LLMs and building generative AI applications.  After that, add agentic AI and MCP. Finally, learn DS fundamentals. This route is suitable for freshers and experienced professionals, even those with non-technical backgrounds. (405.039-558.959)\n",
      "Timestamp: 405.039\n"
     ]
    }
   ],
   "source": [
    "user_input = \"When to learn Modern AU\"\n",
    "CONFIG = {'configurable': {'thread_id': \"newthread\"}}\n",
    "retrieved_chunks = retriever.get_relevant_documents(user_input)\n",
    "context = format_docs(retrieved_chunks)\n",
    "print(\"user :\", user_input)\n",
    "\n",
    "\n",
    "result = workflow.invoke(\n",
    "        {'messages': [HumanMessage(content=user_input)]},\n",
    "        config=CONFIG,\n",
    "    )\n",
    "for msg in result['messages']:\n",
    "    if isinstance(msg, HumanMessage):\n",
    "        if msg.content not in output_dict['human']:\n",
    "            output_dict['human'].append(msg.content)\n",
    "    elif isinstance(msg, AIMessage):\n",
    "        if msg.content not in output_dict['ai']:\n",
    "            output_dict['ai'].append(msg.content)\n",
    "\n",
    "print(\"AI:\", output_dict['ai'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ee1db41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': ['what is this video about', 'When to learn Modern AU'],\n",
       " 'ai': ['Answer: This video is about the roadmap to learn AI in 2025. It includes three different routes: traditional, modern, and advanced.  The speaker discusses what skills are needed for each route and provides free resources and videos.\\nTimestamp: 0.48',\n",
       "  'Answer: This video is about the roadmap to learn AI in 2025. It includes three different routes: traditional, modern, and advanced.  The speaker discusses what each route entails and provides free resources and videos.\\nTimestamp: 0.48',\n",
       "  'Answer: The modern route to learning AI involves mastering generative AI first, focusing on LLMs and building generative AI applications.  After that, add agentic AI and MCP. Finally, learn DS fundamentals. This route is suitable for freshers and experienced professionals, even those with non-technical backgrounds. (405.039-558.959)\\nTimestamp: 405.039']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
