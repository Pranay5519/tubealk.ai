{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88970a28",
   "metadata": {},
   "source": [
    "# Rag Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0834a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "import streamlit as st\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import re      \n",
    "            \n",
    "            \n",
    "# _-----------------------------------------------------FUNCTIONS FOR RAG----------------------------------------------\n",
    "\n",
    "# ------------------ Transcript Loader ------------------\n",
    "def load_transcript(url: str) -> str | None:\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        video_id = match.group(1)\n",
    "        try:\n",
    "            captions = YouTubeTranscriptApi().fetch(video_id).snippets\n",
    "            # join text + start_time\n",
    "            data = [f\"{item.text} ({item.start})\" for item in captions]\n",
    "            return \" \".join(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching transcript: {e}\")\n",
    "            return None\n",
    "\n",
    "# ------------------ Text Splitter ------------------\n",
    "def text_splitter(transcript):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return splitter.create_documents([transcript])\n",
    "\n",
    "# ------------------ Vector Store & Retriever  ------------------\n",
    "def generate_embeddings(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "def retriever_docs(vector_store):\n",
    "    return vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50de37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Imports ------------------\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import TypedDict, Annotated\n",
    "import re\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "#os.environ[\"LANGCHAIN_PROJECT\"] = \"TubeTalkAI Testing\"\n",
    "\n",
    "# ------------------ Build LLM (Gemini) ------------------\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "#--------------------Prompt Template----------------------\n",
    "template = \"\"\"\n",
    "You are the YouTuber from the video, directly answering the viewer’s question.\n",
    "\n",
    "Rules:\n",
    "1. ONLY use the transcript provided below.\n",
    "2. Give the answer in simple, clear sentences — without timestamps inside the text.\n",
    "3. ALWAYS return the exact timestamp (in seconds) from the transcript line you used.\n",
    "   - Do NOT round or estimate timestamps.\n",
    "   - If multiple transcript parts are relevant, return the most direct one.\n",
    "4. Do NOT add greetings, filler, or extra commentary.\n",
    "5. If the transcript does not answer, say: \"Sorry, I didn’t talk about that in this video.\"\n",
    "\n",
    "Transcript:\n",
    "{transcript}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Output format (for schema):\n",
    "- \"answer\": A list of 1–3 short strings that directly answer the question (no timestamps here).\n",
    "- \"timestamps\": The exact timestamp (in seconds) from the transcript where the answer was found.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"transcript\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "# ------------------ Structured Output Schema ------------------\n",
    "class AnsandTime(BaseModel):\n",
    "    answer:str = Field(\n",
    "        description=\"Answers to user's question (do NOT include timestamps here)\"\n",
    "    )\n",
    "    timestamps: float = Field(\n",
    "        description=\"The time (in seconds) from where the answer is taken\"\n",
    "    )\n",
    "\n",
    "structured_model = model.with_structured_output(AnsandTime)\n",
    "\n",
    "# ------------------ ChatState ------------------\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], \"add_messages\"]\n",
    "\n",
    "# ------------------ Chat Node ------------------\n",
    "def chat_node(state: ChatState):\n",
    "    # Extract user question from state\n",
    "    user_message = state[\"messages\"][-1].content  # last message is the user's input\n",
    "\n",
    "    # Fill the prompt\n",
    "    final_prompt = prompt.format(\n",
    "        transcript=context,   # <-- your transcript goes here\n",
    "        question=user_message\n",
    "    )\n",
    "\n",
    "    # Get structured output\n",
    "    response = structured_model.invoke(final_prompt)\n",
    "    ai_text = f\"{response.answer}\\nTimestamp: {response.timestamps}\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            state[\"messages\"][-1],  # include the HumanMessage again\n",
    "            AIMessage(content=ai_text)  # add the AI reply\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# ------------------ Build Graph ------------------\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat_node\", chat_node)\n",
    "graph.add_edge(START, \"chat_node\")\n",
    "graph.add_edge(\"chat_node\", END)\n",
    "\n",
    "CONFIG = {'configurable': {'thread_id': \"newthread\"}}\n",
    "workflow =graph.compile(checkpointer = checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638d3f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript Loaded: Hello all, my name is Krishna Nayak and (0.48) welcome to my YouTube channel. So guys, (2.56) today in this particular video, we are (4.96) going to go ahead and see the entire (7.12) road map to lear ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Load YouTube Transcript ------------------\n",
    "youtube_input = \"https://www.youtube.com/watch?v=s3KnSb9b4Pk\"\n",
    "youtube_captions = load_transcript(youtube_input)\n",
    "print(\"Transcript Loaded:\", youtube_captions[:200], \"...\")\n",
    "\n",
    "# Split & Embed transcript\n",
    "chunks = text_splitter(youtube_captions)\n",
    "vector_store = generate_embeddings(chunks)\n",
    "retriever = retriever_docs(vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1298d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {\"human\": [], \"ai\": []}\n",
    "CONFIG = {'configurable': {'thread_id': \"newthread\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d3e981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user : what is this video about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_17804\\1288449061.py:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_chunks = retriever.get_relevant_documents(user_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: This video is about the entire road map to learn AI in 2025, including free resources, videos, and materials.\n",
      "Timestamp: 9.679\n",
      "user : when to learn Modern AI\n",
      "AI: Nowadays, I usually prefer the modern route for learning AI.\n",
      "Timestamp: 413.44\n",
      "user : how my days does it take to learn Data Science\n",
      "AI: Sorry, I didn’t talk about that in this video.\n",
      "Timestamp: 0.0\n",
      "user : how to many months does it take to learn Data Science\n",
      "AI: It takes four months to learn data science, NLP, and computer vision.\n",
      "Timestamp: 823.36\n",
      "user : how many days does it takes to learn Data Science\n",
      "AI: It takes four months to learn data science, NLP, and computer vision.\n",
      "Timestamp: 823.36\n",
      "user : what are the different Routes present in this Model\n",
      "AI: There are three different routes: the traditional route, the modern route, and the advanced route.\n",
      "Timestamp: 135.84\n",
      "user : what is Advancd Route\n",
      "AI: The advanced route is for really efficient people who are already in the technical domain and can quickly learn things. These people can start all three routes (traditional, modern, and advanced) in parallel.\n",
      "Timestamp: 638.8\n"
     ]
    }
   ],
   "source": [
    "while True : \n",
    "    user_input = input(\"User : \")\n",
    "    if user_input == 'exit':\n",
    "        break\n",
    "    print(\"user :\", user_input)\n",
    "    retrieved_chunks = retriever.get_relevant_documents(user_input)\n",
    "    context = format_docs(retrieved_chunks)\n",
    "    result = workflow.invoke(\n",
    "            {'messages': [HumanMessage(content=user_input)]},\n",
    "            config=CONFIG,\n",
    "        )\n",
    "    for msg in result['messages']:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            if msg.content not in output_dict['human']:\n",
    "                output_dict['human'].append(msg.content)\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            if msg.content not in output_dict['ai']:\n",
    "                output_dict['ai'].append(msg.content)\n",
    "\n",
    "    print(\"AI:\", output_dict['ai'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285a13a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"definitely (123.36) be able to do amazing things over here (125.36) so quickly I will go ahead and share my (127.6) screen so here you can see that I have (129.599) written this amazing road map to learn (131.52) AI in 2025 (133.76) and here I have you know drawn three (135.84) different routes one is the traditional (139.36) route (142.56) The second one is the modern route and (143.84) the third one is something called as an (146.64) advanced route. Now why do we actually (148.64) require this routes? I will discuss (152.56) about this in a much more detailed (154.8) manner. So let's say uh as I said this (156.48) road map actually incorporates for (160.08) everyone whether you are a fresher (162.8) whether you are an experienced (166.0) professional (167.68) whether you are a leader you are in a (169.76) leadership position and whether you are (172.56) a person who is also coming from a (175.519) complete non-technical background. Okay, (177.92) nontechnical background basically\\n\\ntraditional route wherein you're (379.199) mastering everything like data science, (381.44) machine learning, CV and LP. This will (382.96) definitely be your base and on top of (385.44) that you are adding generative AI and (388.16) agentic AI skills. Okay. Now when I'm (390.24) also talking about generative AI and (394.0) agent AI skills, I'm also talking about (395.28) developing end to-end projects, doing (397.199) the deployment in cloud using some (399.12) amazing LLM ops tools, each and (401.12) everything. Okay. Now coming to the (402.88) second part which is the modern route (405.039) right nowadays if many of the people ask (406.8) me kish (409.6) which path should I probably go ahead (411.52) and take in order to learn AI nowadays I (413.44) usually prefer them modern route. Now (416.319) what is modern route? Here I will say (418.0) them hey go ahead and learn (420.24) generative AI first. (424.0) Okay. So here you go ahead and learn (426.479) about generative AI first. In\\n\\nget into the coding (690.64) industry right coding industry then (692.24) definitely follow this pattern that is (695.6) traditional route then go to modern (697.519) route then go to advanced route now you (698.959) you make the decision you know what is (701.279) basically required see wherever you (703.92) learn something let's say if you are (706.399) going to this modern route some amount (707.76) of knowledge is will be required from (709.12) the traditional route also some basic (710.56) knowledge like NLP, machine learning, (712.56) deep learning, you know, that will (714.32) actually set up your base. But it is not (715.92) like you will be stopping over here. If (718.24) you start with this modern route, you'll (720.8) get blocked somewhere. No, here focus (722.48) more on building applications. Now I (724.959) hope you are able to understand from (727.36) this particular road map how you should (730.0) basically go ahead and do this. If I (731.76) will say if you are a\\n\\nyou are in a (169.76) leadership position and whether you are (172.56) a person who is also coming from a (175.519) complete non-technical background. Okay, (177.92) nontechnical background basically means (181.599) let's say that you may be working in HR, (183.44) you may be working in finance and you (186.879) also specifically want to learn how to (188.959) get into AI, right? So I will also be (191.36) talking about them. So in this actual (193.36) route here you can see this is my first (196.64) path traditional route, second part is (198.64) modern route and the third part is (200.959) advanced route. So let me go ahead and (202.48) fill like if someone was coming to me (204.159) around 2 years back and they were asking (207.68) Kish what should be the road map to (209.68) learn data science I would basically say (211.36) okay go ahead and first learn data (213.92) science first. Okay so here I will go (216.4) ahead and write here you have to learn (218.239) data science first.\\n\\n(613.92) how to build those kind of applications. (615.68) Yes, the basic knowledge of NLP, machine (617.36) learning, deep learning can be asked in (620.16) interviews. But nowadays still in this (621.839) modern route, right? If you go for the (624.399) interviews, they're focusing on building (626.0) generative and agentic application, how (628.079) to make rag, how to perform fine-tuning (630.16) each and everything, right? So they're (632.8) focusing more of this specific (634.399) questions. Okay. Now coming to the third (636.079) route. Okay. Which is the advanced route (638.8) who are really really efficient people. (640.959) You know they can start all three (642.72) parallel. All three parallel (646.079) all three parallel. Now these are those (650.8) people who are already in the technical (652.959) domain and they can quickly learn things (654.399) they can start this three right but (657.12) again the focus must be given in this (658.88) modern route more right whenever\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This video is about AI and geometry, and how a non-AI model was already better at geometry than most humans.\n",
      "Timestamp: 80.44\n"
     ]
    }
   ],
   "source": [
    "print(output_dict['ai'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab9281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is this Video About', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='This video is about AI and geometry, and how a non-AI model was already better at geometry than most humans.\\nTimestamp: 80.44', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96103d",
   "metadata": {},
   "source": [
    "# Rag using System Message and MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4836a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import SystemMessage , HumanMessage , AIMessage\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import re      \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI            \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import TypedDict, Annotated\n",
    "import re\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate          \n",
    "# _-----------------------------------------------------FUNCTIONS FOR RAG----------------------------------------------\n",
    "\n",
    "# ------------------ Transcript Loader ------------------\n",
    "def load_transcript(url: str) -> str | None:\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        video_id = match.group(1)\n",
    "        try:\n",
    "            captions = YouTubeTranscriptApi().fetch(video_id).snippets\n",
    "            # join text + start_time\n",
    "            data = [f\"{item.text} ({item.start})\" for item in captions]\n",
    "            return \" \".join(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching transcript: {e}\")\n",
    "            return None\n",
    "\n",
    "# ------------------ Text Splitter ------------------\n",
    "def text_splitter(transcript):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return splitter.create_documents([transcript])\n",
    "\n",
    "# ------------------ Vector Store & Retriever  ------------------\n",
    "def generate_embeddings(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "def retriever_docs(vector_store):\n",
    "    return vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "import os\n",
    "\n",
    "def save_embeddings_faiss(thread_id: str, transcript: str):\n",
    "    # 1. Split transcript\n",
    "    chunks = text_splitter(transcript)\n",
    "\n",
    "    # 2. Build embeddings + FAISS store\n",
    "    vector_store = generate_embeddings(chunks)\n",
    "\n",
    "    # 3. Save FAISS index\n",
    "    save_dir = f\"faiss_indexes/{thread_id}\"\n",
    "    os.makedirs(\"faiss_indexes\", exist_ok=True)\n",
    "    vector_store.save_local(save_dir)\n",
    "\n",
    "    print(f\"✅ Embeddings for {thread_id} saved at {save_dir}\")\n",
    "    \n",
    "def load_embeddings_faiss(thread_id: str):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    save_dir = f\"faiss_indexes/{thread_id}\"\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        raise ValueError(f\"No FAISS index found for thread_id: {thread_id}\")\n",
    "\n",
    "    vector_store = FAISS.load_local(save_dir, embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = retriever_docs(vector_store)\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb027be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "\n",
    "# ------------------ Build LLM (Gemini) ------------------\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
    "\n",
    "# ------------------ System Message ------------------\n",
    "system_message = SystemMessage(content=\"\"\"\n",
    "You are the YouTuber from the video, directly answering the viewer’s question.\n",
    "\n",
    "Rules:\n",
    "1. ONLY use the transcript provided below.\n",
    "2. Give the answer in clear, simple bullet points (not paragraphs).\n",
    "3. Each bullet must include the exact timestamp (in seconds) from the transcript line used.\n",
    "   - Do NOT round or estimate timestamps.\n",
    "   - If multiple transcript parts are relevant, use separate bullets.\n",
    "4. Do NOT add greetings, filler, or extra commentary.\n",
    "5. If the transcript does not answer, say:\n",
    "   - \"Sorry, I didn’t talk about that in this video.\"\n",
    "6. Greet only if the viewer greets first.\n",
    "7. Always remember the viewer’s question when structuring the answer.\n",
    "\"\"\")\n",
    "\n",
    "# ------------------ Structured Schema ------------------\n",
    "class AnsandTime(BaseModel):\n",
    "    answer: list[str] = Field(description=\"Answers to user's question (no timestamps here)\")\n",
    "    timestamps: float = Field(description=\"The time (in seconds) from where the answer was taken\")\n",
    "\n",
    "structured_model = model.with_structured_output(AnsandTime)\n",
    "\n",
    "# ------------------ Chat State ------------------\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "# ------------------ Chat Node ------------------\n",
    "def chat_node(state: ChatState):\n",
    "    user_question = state['messages'][-1].content\n",
    "    \n",
    "    # get context here\n",
    "    retrieved_chunks = retriever.get_relevant_documents(user_question)\n",
    "    context = format_docs(retrieved_chunks)\n",
    "\n",
    "    # build messages\n",
    "    messages = [\n",
    "        system_message,  # rules\n",
    "        SystemMessage(content=f\"Transcript:\\n{context}\"),  # context for model only\n",
    "        HumanMessage(content=user_question)  # clean user input\n",
    "    ]\n",
    "\n",
    "    response = structured_model.invoke(messages)\n",
    "    ai_text = f\"{' '.join(response.answer)}\\nTimestamp: {response.timestamps}\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            state['messages'][-1],       # store user only\n",
    "            AIMessage(content=ai_text)   # store ai only\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a1d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "# ------------------  Checkpointer ------------------\n",
    "conn = sqlite3.connect(database=\"ragDatabase.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn=conn)\n",
    "\n",
    "# ------------------ Build Graph ------------------\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat_node\", chat_node)\n",
    "graph.add_edge(START, \"chat_node\")\n",
    "graph.add_edge(\"chat_node\", END)\n",
    "chatbot = graph.compile(checkpointer=checkpointer)\n",
    "# ------------------ Example Run ------------------\n",
    "thread_id = \"thread5519\"\n",
    "\n",
    "CONFIG = {\"configurable\": {\"thread_id\": thread_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fec43366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings for thread5519 saved at faiss_indexes/thread5519\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Load YouTube Transcript ------------------\n",
    "youtube_input = \"https://www.youtube.com/watch?v=s3KnSb9b4Pk&t\"\n",
    "youtube_captions = load_transcript(youtube_input)\n",
    "\n",
    "save_embeddings_faiss(thread_id=thread_id , transcript= youtube_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71533fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_27624\\163197162.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_chunks = retriever.get_relevant_documents(\"how many days months does it takes to learn DataScience\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"science I would basically say (211.36) okay go ahead and first learn data (213.92) science first. Okay so here I will go (216.4) ahead and write here you have to learn (218.239) data science first. Now what you really (221.68) need to learn in data science. Okay here (224.959) you need to master some amazing things (227.76) like what do you need to master? you (229.84) need to master (232.08) DS that is data science, ML, (233.92) CV, (238.56) NLP. Okay. So here you need to master (240.159) data science, machine learning, computer (243.36) vision, NLP, right? And in all the (244.959) specific topics you should be able to (247.76) understand that stats and linear algebra (250.319) is included a part of it. So in short, (252.959) you're mastering all these technologies (254.959) like data science, machine learning, CV (257.759) and LP. And whenever I'm talking about (259.6) mastering, I'm talking with respect to (261.519) development of a end to-end project. (263.759) Okay. So first of\\n\\n(805.92) three core paths to excellence. Okay. (808.16) And here you can see minimum Python (810.399) 3.11. You can also use 3.9 3. So here (812.88) you have three sections. One is the data (816.16) science and classical AI. Second is (818.56) generative AI. Third is agentic AI. If (820.399) you see path wise data science, NLP, (823.36) computer vision four months I'm taking. (826.639) If you spend two to three hours here, (828.32) you'll be able to definitely become data (830.48) scientist, ML engineer, data analyst. (832.56) Part two is generative LMS. two months (834.72) generative AI engineer, AI product (836.8) developer, agent AI and autonomous (838.48) system, two months a architect, agent (840.399) developer and main thing. This is the (842.24) same route that I have actually (844.48) explained you right now. Okay. Now with (846.24) respect to this, this is my section one. (848.639) I have created a perfect road map to (851.199) learn everything with the free video (853.68)\\n\\nexplained you right now. Okay. Now with (846.24) respect to this, this is my section one. (848.639) I have created a perfect road map to (851.199) learn everything with the free video (853.68) materials and everything itself. Right? (855.76) So here you'll be able to see this (857.6) comprehensive road map covers Python (859.04) programming language stats, machine (860.56) learning, this all you have to do is (861.92) that just go ahead and click this. Okay? (863.6) And if you remember this I have already (865.519) created but I have updated a lot. So (867.44) here you can see work of a data (869.6) scientist data analyst. So first of all (871.199) you need to start with Python (873.12) programming language. All the Python (874.32) programming languages basically over (876.079) here. Then you go ahead with statistics. (877.6) If you start taking the traditional (880.24) route in statistics, I have created very (881.92) detailed videos u in ML 43 videos are (884.56) just there in\\n\\n(1290.72) additional learning resources. (1294.24) See um I'll tell you what is the (1296.559) differences between the free content (1299.039) with respect to the content that is (1300.72) available in the Udemy and live courses (1302.08) also. Okay, as you all know I also sell (1304.159) courses in Udemy in the very affordable (1307.039) cost at 38 3.99. So if you think right (1309.52) you require more help more structured (1313.44) learning then you can just go ahead and (1315.52) follow my Udemy courses. So here I have (1317.12) created another GitHub over here to (1319.76) follow the road map of my Udemy courses. (1322.72) So again based on that track this is (1325.6) much more divided courses like if you (1327.76) want to go ahead and learn mathematics (1329.84) for data science Python with DSA data (1332.0) analyst track data science track (1334.32) generative and engine tracks MLOps and (1336.32) deployment track big data engineering (1339.12) track Figma course track anything\\n\\nthey're saying that hey krish I really (545.04) want to get into AI I also want to start (546.56) coding right so all these things will (548.88) basically be following this modern route (550.88) so it is up to you whether you're in (553.6) experience whether you are 0 to 5 years (555.36) of experience I would still say hey go (557.04) ahead and follow this particular pattern (558.959) right because understand to learn DSML (560.8) CV NLP it will definitely take time if I (563.839) say about this it will actually take (566.72) around 4 months if I talk about geni it (568.72) will take around 2 months only if you (571.36) are devoting around 2 hours daily right (573.68) 2 hours daily so you will definitely be (577.76) able to cover this right here also for (580.0) agentic you may again take 2 months now (582.0) rag and all will come either in genative (585.2) aentic okay Now I hope you are able to (586.959) understand in the modern route because (590.64) we cannot you know right now\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = load_embeddings_faiss(thread_id=thread_id)\n",
    "retrieved_chunks = retriever.get_relevant_documents(\"how many days months does it takes to learn DataScience\")\n",
    "context = format_docs(retrieved_chunks)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7155cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user : what is this video abot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI : This video is about the roadmap to learn AI in 2025. It includes free resources, videos, and materials. The video covers generative AI and agentic AI applications. It provides a roadmap for learning AI within 8 months, even for those with 10+ years of experience and no coding background. The roadmap includes three stages with specific paths and free resources.\n",
      "Timestamp: 2.56\n"
     ]
    }
   ],
   "source": [
    "retriever = load_embeddings_faiss(thread_id=thread_id)\n",
    "while True : \n",
    "    user_input = input(\"User : \")\n",
    "    if user_input == 'exit':\n",
    "        break\n",
    "    print(\"user :\", user_input)\n",
    "    result = chatbot.invoke(\n",
    "            {'messages': [HumanMessage(content=user_input)]},\n",
    "            config=CONFIG,\n",
    "        )\n",
    "    print(\"AI :\" ,result['messages'][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27b7d072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thread1']\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(database=\"ragDatabase.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn=conn)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT DISTINCT thread_id FROM checkpoints\")\n",
    "threads = [row[0] for row in cursor.fetchall()]\n",
    "print(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc1fa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is this video about?', additional_kwargs={}, response_metadata={}, id='d12e1858-e6eb-4f3b-b3fb-661d8c2e5bdf'),\n",
       " AIMessage(content=\"This video is about the roadmap to learn AI in 2025. It provides free resources, videos, and materials. The video covers generative AI and agentic AI applications. It's designed for various backgrounds, including developers, program managers, and leadership positions. A three-stage roadmap is presented, with specific details for each stage. The roadmap includes machine learning, deep learning, NLP, and MLOps. It also covers tools like CircleCI, AI graphana, airflow, bento ML, AWS sagemaker, DVC, and dockers.\\nTimestamp: 2.56\", additional_kwargs={}, response_metadata={}, id='890cad82-a47b-4422-a7d9-3bdb74069839'),\n",
       " HumanMessage(content='when to learn DataScience', additional_kwargs={}, response_metadata={}, id='67d9860d-bf88-44bc-a73a-38ab37fc57a7'),\n",
       " AIMessage(content='Learn data science first To master data science, you need to master data science, machine learning, computer vision, and NLP Stats and linear algebra are included in all specific topics Mastering means developing an end-to-end project\\nTimestamp: 211.36', additional_kwargs={}, response_metadata={}, id='9b72bf79-2da5-49d6-aaa9-2b3ab1f14962'),\n",
       " HumanMessage(content='what are the three main routes in this video', additional_kwargs={}, response_metadata={}, id='ed0cf343-d708-4b75-b45e-f64e0190c271'),\n",
       " AIMessage(content='Traditional route Modern route Advanced route\\nTimestamp: 139.36', additional_kwargs={}, response_metadata={}, id='558686ef-510e-47ba-9368-5b0f05c86ffd')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_conversation(chatbot , thread_id  ):\n",
    "    return chatbot.get_state(config={'configurable': {'thread_id': thread_id}}).values['messages']\n",
    "load_conversation(chatbot=chatbot , thread_id = \"thread1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
