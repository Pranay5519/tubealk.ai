{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1858106",
   "metadata": {},
   "source": [
    "# Basic Summerizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f8f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "load_dotenv()\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f377008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import re\n",
    "def load_transcript(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Fetch transcript for a YouTube video.\n",
    "    \"\"\"\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        video_id = match.group(1)\n",
    "        try:\n",
    "            captions = YouTubeTranscriptApi().fetch(video_id).snippets\n",
    "            data = [f\"{item.text} ({item.start})\" for item in captions]\n",
    "            return \" \".join(data)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching transcript: {e}\")\n",
    "            return None\n",
    "def text_splitter(transcript: str):\n",
    "    \"\"\"\n",
    "    Splits transcript text into chunks for embeddings.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return splitter.create_documents([transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c73cf4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "# ------------------ Structured Schema ------------------\n",
    "class AnsandTime(BaseModel):\n",
    "    Summary: List[str] = Field(description=\"Summarize the YouTube transcript context (no timestamps here)\")\n",
    "    timestamps: float = Field(description=\"The time (in seconds) from where the summary was taken\")\n",
    "\n",
    "structured_model = model.with_structured_output(AnsandTime)\n",
    "system_prompt = \"\"\"\n",
    "You are a summarizer.\n",
    "- Summarize the transcript in simple bullet points (no timestamps in summary).\n",
    "- Also provide the timestamp (in seconds) of where this part of the transcript occurs.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Transcript:\\n{text}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "738a24b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_captions = load_transcript(\"https://www.youtube.com/watch?v=LPZh9BOjkQs\")\n",
    "docs =text_splitter(youtube_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c12cf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_3800\\1609784719.py:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary_result = chain.run(docs)\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 41\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 39\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 35\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Generation\ntext\n  Input should be a valid string [type=string_type, input_value=AnsandTime(Summary=[\"An a...nty.'], timestamps=1.14), input_type=AnsandTime]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m chain = load_summarize_chain(\n\u001b[32m      2\u001b[39m         llm=structured_model,\n\u001b[32m      3\u001b[39m         chain_type=\u001b[33m\"\u001b[39m\u001b[33mmap_reduce\u001b[39m\u001b[33m\"\u001b[39m,   \u001b[38;5;66;03m# or \"refine\", or \"stuff\" if short\u001b[39;00m\n\u001b[32m      4\u001b[39m         map_prompt=prompt,\n\u001b[32m      5\u001b[39m         combine_prompt=prompt\n\u001b[32m      6\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m summary_result = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:190\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    189\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:627\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    625\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33m`run` supports only one positional argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    626\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    628\u001b[39m         _output_key\n\u001b[32m    629\u001b[39m     ]\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    633\u001b[39m         _output_key\n\u001b[32m    634\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:190\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    189\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:410\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    379\u001b[39m \n\u001b[32m    380\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    403\u001b[39m config = {\n\u001b[32m    404\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    405\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    406\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    407\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    408\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    164\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    170\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    171\u001b[39m         inputs,\n\u001b[32m    172\u001b[39m         outputs,\n\u001b[32m    173\u001b[39m         return_only_outputs,\n\u001b[32m    174\u001b[39m     )\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:143\u001b[39m, in \u001b[36mBaseCombineDocumentsChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[32m    142\u001b[39m other_keys = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[38;5;28mself\u001b[39m.input_key}\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m output, extra_return_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m extra_return_dict[\u001b[38;5;28mself\u001b[39m.output_key] = output\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:242\u001b[39m, in \u001b[36mMapReduceDocumentsChain.combine_docs\u001b[39m\u001b[34m(self, docs, token_max, callbacks, **kwargs)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcombine_docs\u001b[39m(\n\u001b[32m    231\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    232\u001b[39m     docs: \u001b[38;5;28mlist\u001b[39m[Document],\n\u001b[32m   (...)\u001b[39m\u001b[32m    235\u001b[39m     **kwargs: Any,\n\u001b[32m    236\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    237\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[32m    238\u001b[39m \n\u001b[32m    239\u001b[39m \u001b[33;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[33;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     map_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     question_result_key = \u001b[38;5;28mself\u001b[39m.llm_chain.output_key\n\u001b[32m    248\u001b[39m     result_docs = [\n\u001b[32m    249\u001b[39m         Document(page_content=r[question_result_key], metadata=docs[i].metadata)\n\u001b[32m    250\u001b[39m         \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[32m    252\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\langchain\\chains\\llm.py:251\u001b[39m, in \u001b[36mLLMChain.apply\u001b[39m\u001b[34m(self, input_list, callbacks)\u001b[39m\n\u001b[32m    245\u001b[39m run_manager = callback_manager.on_chain_start(\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    247\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33minput_list\u001b[39m\u001b[33m\"\u001b[39m: input_list},\n\u001b[32m    248\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.get_name(),\n\u001b[32m    249\u001b[39m )\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    253\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\langchain\\chains\\llm.py:154\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    152\u001b[39m         generations.append([ChatGeneration(message=res)])\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m         generations.append([\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\PROJECTS\\tubetalk.ai\\venv\\Lib\\site-packages\\pydantic\\main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Generation\ntext\n  Input should be a valid string [type=string_type, input_value=AnsandTime(Summary=[\"An a...nty.'], timestamps=1.14), input_type=AnsandTime]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "        llm=structured_model,\n",
    "        chain_type=\"map_reduce\",   # or \"refine\", or \"stuff\" if short\n",
    "        map_prompt=prompt,\n",
    "        combine_prompt=prompt\n",
    "    )\n",
    "summary_result = chain.run(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434940f7",
   "metadata": {},
   "source": [
    "# Summerizer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c951f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "def load_transcript(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Fetch transcript for a YouTube video.\n",
    "    \"\"\"\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        video_id = match.group(1)\n",
    "        try:\n",
    "            captions = YouTubeTranscriptApi().fetch(video_id).snippets\n",
    "            data = [f\"{item.text} ({item.start})\" for item in captions]\n",
    "            return \" \".join(data)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching transcript: {e}\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "def parse_transcript(transcript):\n",
    "    \"\"\"Extract text and timestamps from transcript\"\"\"\n",
    "    pattern = r'(.*?)\\((\\d+\\.?\\d*)\\)'\n",
    "    matches = re.findall(pattern, transcript)\n",
    "    \n",
    "    segments = []\n",
    "    for text, timestamp in matches:\n",
    "        text = text.strip()\n",
    "        if text:\n",
    "            segments.append({\n",
    "                'text': text,\n",
    "                'timestamp': float(timestamp)\n",
    "            })\n",
    "    \n",
    "    return segments\n",
    "def format_for_llm(segments):\n",
    "    \"\"\"Format segments for LLM input\"\"\"\n",
    "    formatted = []\n",
    "    for segment in segments:\n",
    "        formatted.append(f\"[{segment['timestamp']}s] {segment['text']}\")\n",
    "    \n",
    "    return \"\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_captions = load_transcript(\"https://www.youtube.com/watch?v=LPZh9BOjkQs\")\n",
    "segments = parse_transcript(youtube_captions)\n",
    "formatted_transcript = format_for_llm(segments)\n",
    "formatted_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5135626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_video(transcript):\n",
    "    \"\"\"Main function to summarize video transcript\"\"\"\n",
    "    \n",
    "    \"\"\"# Parse transcript\n",
    "    segments = parse_transcript(transcript)\n",
    "    if not segments:\n",
    "        return \"No valid transcript segments found\"\n",
    "    \n",
    "    # Format for LLM\n",
    "    formatted_transcript = format_for_llm(segments)\n",
    "    \"\"\"\n",
    "    # Initialize LLM\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are a video summarizer. Analyze this timestamped transcript and provide:\n",
    "\n",
    "    1. **OVERVIEW**: Brief summary of the video content\n",
    "    2. **KEY POINTS**: Main points with their timestamps (format: \"Point description [timestamp]\")\n",
    "    3. **MAIN TOPICS**: List of topics covered\n",
    "\n",
    "    Transcript:\n",
    "    {transcript}\n",
    "\n",
    "    Format your response clearly with the three sections above.\n",
    "    Always include timestamps in square brackets for each key point.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Generate summary\n",
    "    response = llm.invoke(prompt.format(transcript=transcript))\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95912a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of the video content:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. OVERVIEW\n",
      "The video provides a comprehensive explanation of how large language models (LLMs) function, primarily as next-word predictors. It details the two main phases of their training: an initial \"pre-training\" on vast amounts of text to learn language patterns, followed by \"reinforcement learning with human feedback\" (RLHF) to align their behavior with user preferences. The explanation also covers the underlying technological advancements, particularly the Transformer architecture and its key components like the attention mechanism, which enable the parallel processing and contextual understanding crucial for modern LLMs, highlighting the immense computational scale involved.\n",
      "\n",
      "### 2. KEY POINTS\n",
      "*   Large Language Models (LLMs) are sophisticated mathematical functions that predict what word comes next for any piece of text [33.38].\n",
      "*   Instead of predicting one word with certainty, LLMs assign a probability to all possible next words [44.38].\n",
      "*   Chatbot interactions are built by repeatedly having the model predict the next word in response to user input [51.62].\n",
      "*   Allowing the model to select less likely words randomly makes the output more natural and leads to different answers for the same prompt [73.08].\n",
      "*   Models learn how to make predictions by processing an enormous amount of text, equivalent to over 2600 years of non-stop human reading for GPT-3 [88.04].\n",
      "*   The behavior of a language model is determined by billions of continuous values called parameters or weights, which are tuned during training [108.2].\n",
      "*   Parameters are refined using an algorithm called backpropagation, which tweaks them to make the model more likely to choose the true next word from training examples [158.12].\n",
      "*   The scale of computation involved in training the largest language models is mind-boggling, potentially taking well over 100 million years of continuous operations [189.42].\n",
      "*   After pre-training, chatbots undergo reinforcement learning with human feedback (RLHF) to address the goal of being a good AI assistant by incorporating user preferences [236.88].\n",
      "*   The staggering amount of computation is made possible by special computer chips called GPUs, optimized for running many operations in parallel [254.78].\n",
      "*   The Transformer model, introduced in 2017, revolutionized language models by processing text all at once in parallel, rather than one word at a time [272.08].\n",
      "*   Transformers rely on a special operation called attention, which allows word representations (lists of numbers) to interact and refine their meanings based on context, all in parallel [310.28].\n",
      "*   The specific behavior of LLMs is an emergent phenomenon based on how hundreds of billions of parameters are tuned, making it incredibly challenging to determine why they make exact predictions [388.56].\n",
      "*   The words generated by large language models are uncannily fluent, fascinating, and even useful [408.44].\n",
      "\n",
      "### 3. MAIN TOPICS\n",
      "*   Definition and Function of Large Language Models (LLMs)\n",
      "*   Next-Word Prediction Mechanism\n",
      "*   LLM Training Process (Pre-training)\n",
      "*   Reinforcement Learning with Human Feedback (RLHF)\n",
      "*   Parameters and Scale of LLMs\n",
      "*   Computational Requirements and GPUs\n",
      "*   Transformer Architecture\n",
      "*   Attention Mechanism\n",
      "*   Emergent Behavior of LLMs\n"
     ]
    }
   ],
   "source": [
    "print(summarize_video(youtube_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbcf2eb",
   "metadata": {},
   "source": [
    "# Summerizer 3 advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769573fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import BaseOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "@dataclass\n",
    "class TimestampedSegment:\n",
    "    \"\"\"Represents a segment of transcript with timestamp\"\"\"\n",
    "    text: str\n",
    "    start_time: float\n",
    "    end_time: float = None\n",
    "\n",
    "class SummaryPoint(BaseModel):\n",
    "    \"\"\"Individual summary point with timestamp reference\"\"\"\n",
    "    content: str = Field(description=\"The main point or topic discussed\")\n",
    "    timestamp: float = Field(description=\"Timestamp in seconds when this topic was discussed\")\n",
    "    importance: str = Field(description=\"Importance level: high, medium, low\")\n",
    "\n",
    "class VideoSummary(BaseModel):\n",
    "    \"\"\"Complete video summary structure\"\"\"\n",
    "    title: str = Field(description=\"Suggested title for the video based on content\")\n",
    "    overview: str = Field(description=\"Brief overview of the entire video\")\n",
    "    key_points: List[SummaryPoint] = Field(description=\"List of key points with timestamps\")\n",
    "    main_topics: List[str] = Field(description=\"List of main topics covered\")\n",
    "    duration_summary: str = Field(description=\"Summary of video duration and pacing\")\n",
    "\n",
    "class YouTubeVideoSummarizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the summarizer with OpenAI API key\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenAI API key\n",
    "            model_name: Model to use for summarization\n",
    "        \"\"\"\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.3\n",
    "    )\n",
    "        self.output_parser = PydanticOutputParser(pydantic_object=VideoSummary)\n",
    "    \n",
    "    def parse_transcript(self, transcript: str) -> List[TimestampedSegment]:\n",
    "        \"\"\"\n",
    "        Parse transcript text with timestamps in format:\n",
    "        \"text content (timestamp) more text (timestamp)\"\n",
    "        \n",
    "        Args:\n",
    "            transcript: Raw transcript string\n",
    "            \n",
    "        Returns:\n",
    "            List of TimestampedSegment objects\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        \n",
    "        # Regular expression to find text and timestamps\n",
    "        # Pattern: captures text followed by timestamp in parentheses\n",
    "        pattern = r'(.*?)\\((\\d+\\.?\\d*)\\)'\n",
    "        \n",
    "        matches = re.findall(pattern, transcript)\n",
    "        \n",
    "        for i, (text, timestamp) in enumerate(matches):\n",
    "            text = text.strip()\n",
    "            if text:  # Only add non-empty text segments\n",
    "                segment = TimestampedSegment(\n",
    "                    text=text,\n",
    "                    start_time=float(timestamp),\n",
    "                    end_time=float(matches[i+1][1]) if i+1 < len(matches) else None\n",
    "                )\n",
    "                segments.append(segment)\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def format_transcript_for_llm(self, segments: List[TimestampedSegment]) -> str:\n",
    "        \"\"\"\n",
    "        Format parsed segments for LLM processing\n",
    "        \n",
    "        Args:\n",
    "            segments: List of timestamped segments\n",
    "            \n",
    "        Returns:\n",
    "            Formatted transcript string\n",
    "        \"\"\"\n",
    "        formatted_parts = []\n",
    "        \n",
    "        for segment in segments:\n",
    "            timestamp_str = f\"[{segment.start_time}s]\"\n",
    "            formatted_parts.append(f\"{timestamp_str} {segment.text}\")\n",
    "        \n",
    "        return \"\\n\".join(formatted_parts)\n",
    "    \n",
    "    def create_summary_prompt(self) -> ChatPromptTemplate:\n",
    "        \"\"\"Create the prompt template for video summarization\"\"\"\n",
    "        \n",
    "        system_message = SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"You are an expert video content analyzer and summarizer. \n",
    "            You will receive a transcript of a YouTube video with timestamps.\n",
    "            \n",
    "            Your task is to:\n",
    "            1. Create a structured summary with key points\n",
    "            2. Include timestamp references for each point\n",
    "            3. Identify main topics and themes\n",
    "            4. Provide an overall assessment of the video content\n",
    "            \n",
    "            Be concise but comprehensive. Focus on the most important information.\n",
    "            Always include the timestamp reference for where you found each piece of information.\n",
    "            \n",
    "            {format_instructions}\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        human_message = HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"Please analyze and summarize the following timestamped video transcript:\n",
    "\n",
    "            {transcript}\n",
    "            \n",
    "            Provide a structured summary following the specified format.\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        return ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    \n",
    "    def summarize_video(self, transcript: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main method to summarize a video transcript\n",
    "        \n",
    "        Args:\n",
    "            transcript: Raw transcript string with timestamps\n",
    "            \n",
    "        Returns:\n",
    "            Structured summary dictionary\n",
    "        \"\"\"\n",
    "        # Parse the transcript\n",
    "        segments = self.parse_transcript(transcript)\n",
    "        \n",
    "        if not segments:\n",
    "            return {\"error\": \"No valid transcript segments found\"}\n",
    "        \n",
    "        # Format for LLM\n",
    "        formatted_transcript = self.format_transcript_for_llm(segments)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = self.create_summary_prompt()\n",
    "        \n",
    "        # Prepare the input\n",
    "        formatted_prompt = prompt.format_prompt(\n",
    "            transcript=formatted_transcript,\n",
    "            format_instructions=self.output_parser.get_format_instructions()\n",
    "        )\n",
    "        \n",
    "        # Generate summary\n",
    "        try:\n",
    "            response = self.llm(formatted_prompt.to_messages())\n",
    "            parsed_output = self.output_parser.parse(response.content)\n",
    "            \n",
    "            # Convert to dictionary and add metadata\n",
    "            summary_dict = parsed_output.dict()\n",
    "            summary_dict[\"total_segments\"] = len(segments)\n",
    "            summary_dict[\"video_duration\"] = max([s.start_time for s in segments]) if segments else 0\n",
    "            \n",
    "            return response,parsed_output , summary_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to generate summary: {str(e)}\"}\n",
    "    def format_summary_output(self, summary: Dict[str, Any]) -> str:\n",
    "        if \"error\" in summary:\n",
    "            return f\"Error: {summary['error']}\"\n",
    "        \n",
    "        output = []\n",
    "        output.append(f\"üìπ VIDEO SUMMARY\")\n",
    "        output.append(\"=\" * 50)\n",
    "        output.append(f\"Title: {summary['title']}\")\n",
    "        output.append(f\"Duration: {summary.get('video_duration', 0):.1f} seconds\")\n",
    "        output.append(f\"Total Segments: {summary.get('total_segments', 0)}\")\n",
    "        output.append(\"\")\n",
    "        \n",
    "        output.append(\"üìã OVERVIEW:\")\n",
    "        output.append(summary['overview'])\n",
    "        output.append(\"\")\n",
    "        \n",
    "        output.append(\"üéØ KEY POINTS:\")\n",
    "        for i, point in enumerate(summary['key_points'], 1):\n",
    "            output.append(f\"{i}. {point['content']}\")\n",
    "            output.append(f\"   ‚è∞ Timestamp: {point['timestamp']}s | Importance: {point['importance']}\")\n",
    "            output.append(\"\")\n",
    "        \n",
    "        output.append(\"üìö MAIN TOPICS:\")\n",
    "        for topic in summary['main_topics']:\n",
    "            output.append(f\"‚Ä¢ {topic}\")\n",
    "        output.append(\"\")\n",
    "        \n",
    "        output.append(\"‚è±Ô∏è PACING ANALYSIS:\")\n",
    "        output.append(summary['duration_summary'])\n",
    "        \n",
    "        return \"\\n\".join(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436a3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Segments:\n",
      "[1.14s] Imagine you happen across a short movie script that\n",
      "[3.976s] describes a scene between a person and their AI assistant.\n",
      "[7.48s] The script has what the person asks the AI, but the AI's response has been torn off.\n",
      "[13.06s] Suppose you also have this powerful magical machine that can take\n",
      "[16.98s] any text and provide a sensible prediction of what word comes next.\n",
      "[21.5s] You could then finish the script by feeding in what you have to the machine,\n",
      "[25.506s] seeing what it would predict to start the AI's answer,\n",
      "[28.368s] and then repeating this over and over with a growing script completing the dialogue.\n",
      "[33.38s] When you interact with a chatbot, this is exactly what's happening.\n",
      "[37.02s] A large language model is a sophisticated mathematical function\n",
      "[40.701s] that predicts what word comes next for any piece of text.\n",
      "[44.38s] Instead of predicting one word with certainty, though,\n",
      "[47.402s] what it does is assign a probability to all possible next words.\n",
      "[51.62s] To build a chatbot, you lay out some text that describes an interaction between a user\n",
      "[56.8s] and a hypothetical AI assistant, add on whatever the user types in as the first part of\n",
      "[62.04s] the interaction, and then have the model repeatedly predict the next word that such a\n",
      "[67.16s] hypothetical AI assistant would say in response, and that's what's presented to the user.\n",
      "[73.08s] In doing this, the output tends to look a lot more natural if\n",
      "[76.214s] you allow it to select less likely words along the way at random.\n",
      "[80.14s] So what this means is even though the model itself is deterministic,\n",
      "[83.62s] a given prompt typically gives a different answer each time it's run.\n",
      "[88.04s] Models learn how to make these predictions by processing an enormous amount of text,\n",
      "[92.332s] typically pulled from the internet.\n",
      "[94.1s] For a standard human to read the amount of text that was used to train GPT-3,\n",
      "[99.471s] for example, if they read non-stop 24-7, it would take over 2600 years.\n",
      "[104.72s] Larger models since then train on much, much more.\n",
      "[108.2s] You can think of training a little bit like tuning the dials on a big machine.\n",
      "[112.28s] The way that a language model behaves is entirely determined by these\n",
      "[116.301s] many different continuous values, usually called parameters or weights.\n",
      "[121.02s] Changing those parameters will change the probabilities\n",
      "[124.099s] that the model gives for the next word on a given input.\n",
      "[127.86s] What puts the large in large language model is how\n",
      "[130.727s] they can have hundreds of billions of these parameters.\n",
      "[135.2s] No human ever deliberately sets those parameters.\n",
      "[138.44s] Instead, they begin at random, meaning the model just outputs gibberish,\n",
      "[142.643s] but they're repeatedly refined based on many example pieces of text.\n",
      "[147.14s] One of these training examples could be just a handful of words,\n",
      "[150.656s] or it could be thousands, but in either case, the way this works is to\n",
      "[154.496s] pass in all but the last word from that example into the model and\n",
      "[158.12s] compare the prediction that it makes with the true last word from the example.\n",
      "[163.26s] An algorithm called backpropagation is used to tweak all of the parameters\n",
      "[167.393s] in such a way that it makes the model a little more likely to choose\n",
      "[171.196s] the true last word and a little less likely to choose all the others.\n",
      "[175.74s] When you do this for many, many trillions of examples,\n",
      "[178.75s] not only does the model start to give more accurate predictions on the training data,\n",
      "[183.458s] but it also starts to make more reasonable predictions on text that it's never\n",
      "[187.783s] seen before.\n",
      "[189.42s] Given the huge number of parameters and the enormous amount of training data,\n",
      "[193.919s] the scale of computation involved in training a large language model is mind-boggling.\n",
      "[199.6s] To illustrate, imagine that you could perform one\n",
      "[202.285s] billion additions and multiplications every single second.\n",
      "[206.06s] How long do you think it would take for you to do all of the\n",
      "[209.326s] operations involved in training the largest language models?\n",
      "[213.46s] Do you think it would take a year?\n",
      "[216.039s] Maybe something like 10,000 years?\n",
      "[219.02s] The answer is actually much more than that.\n",
      "[221.12s] It's well over 100 million years.\n",
      "[225.52s] This is only part of the story, though.\n",
      "[227.54s] This whole process is called pre-training.\n",
      "[229.5s] The goal of auto-completing a random passage of text from the\n",
      "[232.646s] internet is very different from the goal of being a good AI assistant.\n",
      "[236.88s] To address this, chatbots undergo another type of training,\n",
      "[240.08s] just as important, called reinforcement learning with human feedback.\n",
      "[244.48s] Workers flag unhelpful or problematic predictions,\n",
      "[247.498s] and their corrections further change the model's parameters,\n",
      "[251.109s] making them more likely to give predictions that users prefer.\n",
      "[254.78s] Looking back at the pre-training, though, this staggering amount of\n",
      "[258.86s] computation is only made possible by using special computer chips that\n",
      "[263.12s] are optimized for running many operations in parallel, known as GPUs.\n",
      "[268.12s] However, not all language models can be easily parallelized.\n",
      "[272.08s] Prior to 2017, most language models would process text one word at a time,\n",
      "[276.817s] but then a team of researchers at Google introduced a new model known as the transformer.\n",
      "[283.3s] Transformers don't read text from the start to the finish,\n",
      "[286.745s] they soak it all in at once, in parallel.\n",
      "[289.9s] The very first step inside a transformer, and most other language models for that matter,\n",
      "[294.6s] is to associate each word with a long list of numbers.\n",
      "[297.86s] The reason for this is that the training process only works with continuous values,\n",
      "[302.396s] so you have to somehow encode language using numbers,\n",
      "[305.312s] and each of these lists of numbers may somehow encode the meaning of the\n",
      "[309.254s] corresponding word.\n",
      "[310.28s] What makes transformers unique is their reliance\n",
      "[313.36s] on a special operation known as attention.\n",
      "[316.98s] This operation gives all of these lists of numbers a chance to talk to one another\n",
      "[321.684s] and refine the meanings they encode based on the context around, all done in parallel.\n",
      "[327.4s] For example, the numbers encoding the word bank might be changed based on the\n",
      "[331.707s] context surrounding it to somehow encode the more specific notion of a riverbank.\n",
      "[337.28s] Transformers typically also include a second type of operation known\n",
      "[341.029s] as a feed-forward neural network, and this gives the model extra\n",
      "[344.561s] capacity to store more patterns about language learned during training.\n",
      "[349.28s] All of this data repeatedly flows through many different iterations of\n",
      "[353.401s] these two fundamental operations, and as it does so,\n",
      "[356.478s] the hope is that each list of numbers is enriched to encode whatever\n",
      "[360.484s] information might be needed to make an accurate prediction of what word\n",
      "[364.664s] follows in the passage.\n",
      "[367.0s] At the end, one final function is performed on the last vector in this sequence,\n",
      "[371.534s] which now has had a chance to be influenced by all the other context from the input text,\n",
      "[376.573s] as well as everything the model learned during training,\n",
      "[379.764s] to produce a prediction of the next word.\n",
      "[382.48s] Again, the model's prediction looks like a probability for every possible next word.\n",
      "[388.56s] Although researchers design the framework for how each of these steps work,\n",
      "[392.794s] it's important to understand that the specific behavior is an emergent phenomenon\n",
      "[397.362s] based on how those hundreds of billions of parameters are tuned during training.\n",
      "[402.48s] This makes it incredibly challenging to determine\n",
      "[405.07s] why the model makes the exact predictions that it does.\n",
      "[408.44s] What you can see is that when you use large language model predictions to autocomplete\n",
      "[413.778s] a prompt, the words that it generates are uncannily fluent, fascinating, and even useful.\n",
      "[425.719s] If you're a new viewer and you're curious about more details on how\n",
      "[428.826s] transformers and attention work, boy do I have some material for you.\n",
      "[432.399s] One option is to jump into a series I made about deep learning,\n",
      "[436.08s] where we visualize and motivate the details of attention and all the other steps\n",
      "[440.74s] in a transformer.\n",
      "[442.099s] Also, on my second channel I just posted a talk I gave a couple\n",
      "[445.529s] months ago about this topic for the company TNG in Munich.\n",
      "[449.079s] Sometimes I actually prefer the content I make as a casual talk rather than a produced\n",
      "[453.101s] video, but I leave it up to you which one of these feels like the better follow-on.\n",
      "\n",
      "==================================================\n",
      "Formatted for LLM:\n",
      "[1.14s] Imagine you happen across a short movie script that\n",
      "[3.976s] describes a scene between a person and their AI assistant.\n",
      "[7.48s] The script has what the person asks the AI, but the AI's response has been torn off.\n",
      "[13.06s] Suppose you also have this powerful magical machine that can take\n",
      "[16.98s] any text and provide a sensible prediction of what word comes next.\n",
      "[21.5s] You could then finish the script by feeding in what you have to the machine,\n",
      "[25.506s] seeing what it would predict to start the AI's answer,\n",
      "[28.368s] and then repeating this over and over with a growing script completing the dialogue.\n",
      "[33.38s] When you interact with a chatbot, this is exactly what's happening.\n",
      "[37.02s] A large language model is a sophisticated mathematical function\n",
      "[40.701s] that predicts what word comes next for any piece of text.\n",
      "[44.38s] Instead of predicting one word with certainty, though,\n",
      "[47.402s] what it does is assign a probability to all possible next words.\n",
      "[51.62s] To build a chatbot, you lay out some text that describes an interaction between a user\n",
      "[56.8s] and a hypothetical AI assistant, add on whatever the user types in as the first part of\n",
      "[62.04s] the interaction, and then have the model repeatedly predict the next word that such a\n",
      "[67.16s] hypothetical AI assistant would say in response, and that's what's presented to the user.\n",
      "[73.08s] In doing this, the output tends to look a lot more natural if\n",
      "[76.214s] you allow it to select less likely words along the way at random.\n",
      "[80.14s] So what this means is even though the model itself is deterministic,\n",
      "[83.62s] a given prompt typically gives a different answer each time it's run.\n",
      "[88.04s] Models learn how to make these predictions by processing an enormous amount of text,\n",
      "[92.332s] typically pulled from the internet.\n",
      "[94.1s] For a standard human to read the amount of text that was used to train GPT-3,\n",
      "[99.471s] for example, if they read non-stop 24-7, it would take over 2600 years.\n",
      "[104.72s] Larger models since then train on much, much more.\n",
      "[108.2s] You can think of training a little bit like tuning the dials on a big machine.\n",
      "[112.28s] The way that a language model behaves is entirely determined by these\n",
      "[116.301s] many different continuous values, usually called parameters or weights.\n",
      "[121.02s] Changing those parameters will change the probabilities\n",
      "[124.099s] that the model gives for the next word on a given input.\n",
      "[127.86s] What puts the large in large language model is how\n",
      "[130.727s] they can have hundreds of billions of these parameters.\n",
      "[135.2s] No human ever deliberately sets those parameters.\n",
      "[138.44s] Instead, they begin at random, meaning the model just outputs gibberish,\n",
      "[142.643s] but they're repeatedly refined based on many example pieces of text.\n",
      "[147.14s] One of these training examples could be just a handful of words,\n",
      "[150.656s] or it could be thousands, but in either case, the way this works is to\n",
      "[154.496s] pass in all but the last word from that example into the model and\n",
      "[158.12s] compare the prediction that it makes with the true last word from the example.\n",
      "[163.26s] An algorithm called backpropagation is used to tweak all of the parameters\n",
      "[167.393s] in such a way that it makes the model a little more likely to choose\n",
      "[171.196s] the true last word and a little less likely to choose all the others.\n",
      "[175.74s] When you do this for many, many trillions of examples,\n",
      "[178.75s] not only does the model start to give more accurate predictions on the training data,\n",
      "[183.458s] but it also starts to make more reasonable predictions on text that it's never\n",
      "[187.783s] seen before.\n",
      "[189.42s] Given the huge number of parameters and the enormous amount of training data,\n",
      "[193.919s] the scale of computation involved in training a large language model is mind-boggling.\n",
      "[199.6s] To illustrate, imagine that you could perform one\n",
      "[202.285s] billion additions and multiplications every single second.\n",
      "[206.06s] How long do you think it would take for you to do all of the\n",
      "[209.326s] operations involved in training the largest language models?\n",
      "[213.46s] Do you think it would take a year?\n",
      "[216.039s] Maybe something like 10,000 years?\n",
      "[219.02s] The answer is actually much more than that.\n",
      "[221.12s] It's well over 100 million years.\n",
      "[225.52s] This is only part of the story, though.\n",
      "[227.54s] This whole process is called pre-training.\n",
      "[229.5s] The goal of auto-completing a random passage of text from the\n",
      "[232.646s] internet is very different from the goal of being a good AI assistant.\n",
      "[236.88s] To address this, chatbots undergo another type of training,\n",
      "[240.08s] just as important, called reinforcement learning with human feedback.\n",
      "[244.48s] Workers flag unhelpful or problematic predictions,\n",
      "[247.498s] and their corrections further change the model's parameters,\n",
      "[251.109s] making them more likely to give predictions that users prefer.\n",
      "[254.78s] Looking back at the pre-training, though, this staggering amount of\n",
      "[258.86s] computation is only made possible by using special computer chips that\n",
      "[263.12s] are optimized for running many operations in parallel, known as GPUs.\n",
      "[268.12s] However, not all language models can be easily parallelized.\n",
      "[272.08s] Prior to 2017, most language models would process text one word at a time,\n",
      "[276.817s] but then a team of researchers at Google introduced a new model known as the transformer.\n",
      "[283.3s] Transformers don't read text from the start to the finish,\n",
      "[286.745s] they soak it all in at once, in parallel.\n",
      "[289.9s] The very first step inside a transformer, and most other language models for that matter,\n",
      "[294.6s] is to associate each word with a long list of numbers.\n",
      "[297.86s] The reason for this is that the training process only works with continuous values,\n",
      "[302.396s] so you have to somehow encode language using numbers,\n",
      "[305.312s] and each of these lists of numbers may somehow encode the meaning of the\n",
      "[309.254s] corresponding word.\n",
      "[310.28s] What makes transformers unique is their reliance\n",
      "[313.36s] on a special operation known as attention.\n",
      "[316.98s] This operation gives all of these lists of numbers a chance to talk to one another\n",
      "[321.684s] and refine the meanings they encode based on the context around, all done in parallel.\n",
      "[327.4s] For example, the numbers encoding the word bank might be changed based on the\n",
      "[331.707s] context surrounding it to somehow encode the more specific notion of a riverbank.\n",
      "[337.28s] Transformers typically also include a second type of operation known\n",
      "[341.029s] as a feed-forward neural network, and this gives the model extra\n",
      "[344.561s] capacity to store more patterns about language learned during training.\n",
      "[349.28s] All of this data repeatedly flows through many different iterations of\n",
      "[353.401s] these two fundamental operations, and as it does so,\n",
      "[356.478s] the hope is that each list of numbers is enriched to encode whatever\n",
      "[360.484s] information might be needed to make an accurate prediction of what word\n",
      "[364.664s] follows in the passage.\n",
      "[367.0s] At the end, one final function is performed on the last vector in this sequence,\n",
      "[371.534s] which now has had a chance to be influenced by all the other context from the input text,\n",
      "[376.573s] as well as everything the model learned during training,\n",
      "[379.764s] to produce a prediction of the next word.\n",
      "[382.48s] Again, the model's prediction looks like a probability for every possible next word.\n",
      "[388.56s] Although researchers design the framework for how each of these steps work,\n",
      "[392.794s] it's important to understand that the specific behavior is an emergent phenomenon\n",
      "[397.362s] based on how those hundreds of billions of parameters are tuned during training.\n",
      "[402.48s] This makes it incredibly challenging to determine\n",
      "[405.07s] why the model makes the exact predictions that it does.\n",
      "[408.44s] What you can see is that when you use large language model predictions to autocomplete\n",
      "[413.778s] a prompt, the words that it generates are uncannily fluent, fascinating, and even useful.\n",
      "[425.719s] If you're a new viewer and you're curious about more details on how\n",
      "[428.826s] transformers and attention work, boy do I have some material for you.\n",
      "[432.399s] One option is to jump into a series I made about deep learning,\n",
      "[436.08s] where we visualize and motivate the details of attention and all the other steps\n",
      "[440.74s] in a transformer.\n",
      "[442.099s] Also, on my second channel I just posted a talk I gave a couple\n",
      "[445.529s] months ago about this topic for the company TNG in Munich.\n",
      "[449.079s] Sometimes I actually prefer the content I make as a casual talk rather than a produced\n",
      "[453.101s] video, but I leave it up to you which one of these feels like the better follow-on.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_23764\\1126811240.py:163: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  summary_dict = parsed_output.dict()\n"
     ]
    }
   ],
   "source": [
    "def load_transcript(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Fetch transcript for a YouTube video.\n",
    "    \"\"\"\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        video_id = match.group(1)\n",
    "        try:\n",
    "            captions = YouTubeTranscriptApi().fetch(video_id).snippets\n",
    "            data = [f\"{item.text} ({item.start})\" for item in captions]\n",
    "            return \" \".join(data)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching transcript: {e}\")\n",
    "            return None\n",
    "sample_transcript =load_transcript(\"https://www.youtube.com/watch?v=LPZh9BOjkQs\")\n",
    "\n",
    "# Initialize the summarizer (you'll need to provide your OpenAI API key)\n",
    "summarizer = YouTubeVideoSummarizer()\n",
    "\n",
    "# Test transcript parsing\n",
    "segments = summarizer.parse_transcript(sample_transcript)\n",
    "print(\"Parsed Segments:\")\n",
    "for segment in segments:\n",
    "    print(f\"[{segment.start_time}s] {segment.text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Formatted for LLM:\")\n",
    "formatted = summarizer.format_transcript_for_llm(segments)\n",
    "print(formatted)\n",
    "\n",
    "# Generate summary (uncomment when you have API key)\n",
    "response , parsed_output , summary = summarizer.summarize_video(sample_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d30915ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(summarizer.format_summary_output(summary=summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86eba644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=33&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=37&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=88&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=108&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=199&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=227&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=254&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=276&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=289&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=388&autoplay=1\n"
     ]
    }
   ],
   "source": [
    "for  output in parsed_output.key_points:\n",
    "    timestamp = output.timestamp\n",
    "    timestamp_url_play = f\"{embed_url}?start={int(float(timestamp))}&autoplay=1\"\n",
    "    print(timestamp_url_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a78cca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=33&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=37&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=88&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=108&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=127&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=227&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=254&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=276&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=382&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=392&autoplay=1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_url =\"https://www.youtube.com/embed/LPZh9BOjkQs\"\n",
    "for x in summary['key_points']:\n",
    "    timestamp_url_play = f\"{embed_url}?start={int(float(x['timestamp']))}&autoplay=1\"\n",
    "    print(timestamp_url_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c3a65a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìπ VIDEO SUMMARY\n",
      "==================================================\n",
      "Title: How Large Language Models (LLMs) Power Chatbots: A Deep Dive\n",
      "Duration: 453.1 seconds\n",
      "Total Segments: 115\n",
      "\n",
      "üìã OVERVIEW:\n",
      "This video explains how large language models (LLMs) work, focusing on their architecture (transformers), training process (pre-training and reinforcement learning), and the massive computational resources required.  It also touches upon the probabilistic nature of their predictions and the challenges in understanding their decision-making.\n",
      "\n",
      "üéØ KEY POINTS:\n",
      "1. Chatbots function by repeatedly predicting the next word in a conversation using a large language model (LLM).\n",
      "   ‚è∞ Timestamp: 33.38s | Importance: high\n",
      "\n",
      "2. LLMs are sophisticated mathematical functions that assign probabilities to all possible next words in a text sequence.\n",
      "   ‚è∞ Timestamp: 37.02s | Importance: high\n",
      "\n",
      "3. LLMs are trained on massive datasets of text from the internet, requiring immense computational power (over 100 million years of computation for the largest models).\n",
      "   ‚è∞ Timestamp: 88.04s | Importance: high\n",
      "\n",
      "4. Training involves adjusting parameters (weights) to improve the model's ability to predict the next word in a given sequence using backpropagation.\n",
      "   ‚è∞ Timestamp: 108.2s | Importance: high\n",
      "\n",
      "5. The 'large' in LLM refers to the hundreds of billions of parameters that are adjusted during training, not set manually.\n",
      "   ‚è∞ Timestamp: 127.86s | Importance: medium\n",
      "\n",
      "6. LLMs undergo pre-training on vast text datasets and then reinforcement learning with human feedback to refine their responses and make them more helpful and user-friendly.\n",
      "   ‚è∞ Timestamp: 227.54s | Importance: high\n",
      "\n",
      "7. The use of GPUs enables the parallelization of computations, crucial for training LLMs efficiently.\n",
      "   ‚è∞ Timestamp: 254.78s | Importance: medium\n",
      "\n",
      "8. Transformers, a key architecture in LLMs, process text in parallel using 'attention' mechanisms to consider the context of all words simultaneously.\n",
      "   ‚è∞ Timestamp: 276.817s | Importance: high\n",
      "\n",
      "9. The final prediction of the LLM is a probability distribution over all possible next words.\n",
      "   ‚è∞ Timestamp: 382.48s | Importance: medium\n",
      "\n",
      "10. The specific behavior of LLMs is an emergent property of their parameters, making it difficult to understand their exact decision-making process.\n",
      "   ‚è∞ Timestamp: 392.794s | Importance: medium\n",
      "\n",
      "üìö MAIN TOPICS:\n",
      "‚Ä¢ Large Language Models (LLMs)\n",
      "‚Ä¢ Chatbot Functionality\n",
      "‚Ä¢ Training Process (Pre-training & Reinforcement Learning)\n",
      "‚Ä¢ Transformer Architecture\n",
      "‚Ä¢ Computational Requirements\n",
      "‚Ä¢ Probabilistic Nature of Predictions\n",
      "\n",
      "‚è±Ô∏è PACING ANALYSIS:\n",
      "The video provides a comprehensive explanation of LLMs and chatbots, covering various aspects in a well-paced manner.  The length is appropriate for the depth of the subject matter.\n"
     ]
    }
   ],
   "source": [
    "def format_summary_output(summary: Dict[str, Any]) -> str:\n",
    "        if \"error\" in summary:\n",
    "            return f\"Error: {summary['error']}\"\n",
    "        \n",
    "        output = []\n",
    "        output.append(f\"üìπ VIDEO SUMMARY\")\n",
    "        output.append(\"=\" * 50)\n",
    "        output.append(f\"Title: {summary['title']}\")\n",
    "        output.append(f\"Duration: {summary.get('video_duration', 0):.1f} seconds\")\n",
    "        output.append(f\"Total Segments: {summary.get('total_segments', 0)}\")\n",
    "        output.append(\"\")\n",
    "        \n",
    "        output.append(\"üìã OVERVIEW:\")\n",
    "        output.append(summary['overview'])\n",
    "        output.append(\"\")\n",
    "        \n",
    "        output.append(\"üéØ KEY POINTS:\")\n",
    "        for i, point in enumerate(summary['key_points'], 1):\n",
    "            output.append(f\"{i}. {point['content']}\")\n",
    "            output.append(f\"   ‚è∞ Timestamp: {point['timestamp']}s | Importance: {point['importance']}\")\n",
    "            output.append(\"\")\n",
    "        \n",
    "        output.append(\"üìö MAIN TOPICS:\")\n",
    "        for topic in summary['main_topics']:\n",
    "            output.append(f\"‚Ä¢ {topic}\")\n",
    "        output.append(\"\")\n",
    "        \n",
    "        output.append(\"‚è±Ô∏è PACING ANALYSIS:\")\n",
    "        output.append(summary['duration_summary'])\n",
    "        \n",
    "        return \"\\n\".join(output)\n",
    "print(format_summary_output(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b96d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dddaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_url =\"https://www.youtube.com/embed/LPZh9BOjkQs\"\n",
    "for x in summary['key_points']:\n",
    "    timestamp_url_play = f\"{embed_url}?start={int(float(x['timestamp']))}&autoplay=1\"\n",
    "    print(timestamp_url_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60f404c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=33&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=37&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=88&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=108&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=127&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=227&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=254&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=276&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=382&autoplay=1\n",
      "https://www.youtube.com/embed/LPZh9BOjkQs?start=392&autoplay=1\n"
     ]
    }
   ],
   "source": [
    "import streamlit as  st\n",
    "embed_url =\"https://www.youtube.com/embed/LPZh9BOjkQs\"\n",
    "for i, point in enumerate(summary['key_points'], 1):\n",
    "    timestamp_url_play = f\"{embed_url}?start={int(float(point['timestamp']))}&autoplay=1\"\n",
    "    print(timestamp_url_play)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
